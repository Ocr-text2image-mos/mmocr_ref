Title: 'Perceiving Stroke-Semantic Context: Hierarchical Contrastive Learning for Robust Scene Text Recognition'
Abbreviation: PerSec
Venue: AAAI
Year: 2022
Lab/Company:
 - Tencent YouTu Lab
 - University of Science and Technology of China
URL:
  Venue: 'https://www.aaai.org/AAAI22Papers/AAAI-785.LiuH.pdf'
  Arxiv: N/A
Bibtex: '@inproceedings{liu2022perceiving,
  title={Perceiving Stroke-Semantic Context: Hierarchical Contrastive Learning for Robust Scene Text Recognition},
  author={Liu, Hao and Wang, Bin and Bao, Zhimin and Xue, Mobai and Kang, Sheng and Jiang, Deqiang and Liu, Yinsong and Ren, Bo},
  year={2022},
  organization={AAAI}}'
Code: N/A

Experiments:
  Name: PerSec
  Metadata:
    Device: N/A
    InferenceTime: 100
    FLOPs: 30
    Params: 20
    Training Data: MJ\ST\SA\Real
  Results:
    - Test Data: IIIT5K
      Metrics:
        WAICS: 88.1
    - Test Data: SVT
      Metrics:
        WAICS: 96.7
    - Test Data: IC13
      Metrics:
        WAICS: 73.6
    - Test Data: IC15
      Metrics:
        WAICS: 77.7
    - Test Data: SVTP
      Metrics:
        WAICS: 72.7
    - Test Data: CUTE
      Metrics:
        WAICS: 83.8


Abstract: 'We introduce Perceiving Stroke-Semantic Context (PerSec), a new
approach to self-supervised representation learning tailored for Scene Text
Recognition (STR) task. Considering scene text images carry both visual and
semantic properties, we equip our PerSec with dual context perceivers which
can contrast and learn latent representations from low-level stroke and
high-level semantic contextual spaces simultaneously via hierarchical
contrastive learning on unlabeled text image data. Experiments in un- and
semi-supervised learning settings on STR benchmarks demonstrate our
proposed framework can yield a more robust representation for both
CTC-based and attention-based decoders than other contrastive learning
methods. To fully investigate the potential of our method, we also
collect a dataset of 100 million unlabeled text images, named UTI-100M,
covering 5 scenes and 4 languages. By leveraging hundred-million-level
unlabeled data, our PerSec shows significant performance improvement
when fine-tuning the learned representation on the labeled data.
Furthermore, we observe that the representation learned by PerSec
presents great generalization, especially under few labeled data scenes.'

Network Structure: 'https://user-images.githubusercontent.com/24622904/211475019-ac656c48-3bc0-41e7-ae23-6aa8be8e0287.png'


Architecture:
  - CTC
  - Attention
  - Transformer
Learning Method:
  - Self-Supervised
  - Supervised
Language Modality:
  - Implicit Language Model

Paper Reading URL: 'https://mp.weixin.qq.com/s?__biz=MzI1ODk1ODI5Mw==&mid=2247489751&idx=1&sn=38430279107d2a53827adec7884b9ce2&chksm=ea016e6ddd76e77b5ecdafc8bffd57da538751e273147fa3706e5d22e0385f01d446bdb031d0&scene=126&&sessionid=1670397988#rd'
